{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building the RAG Query Flow\n",
    "\n",
    "This lab walks you through the core steps of a Retrieval Augmented Generation (RAG) query. We'll use Voyage-AI to embed a user's question, then query MongoDB Atlas Vector Search to retrieve relevant context, and finally, call Azure OpenAI via LangChain to generate an answer.\n",
    "\n",
    "## Objectives\n",
    "- Embed a user's query using Voyage-AI.\n",
    "- Perform a vector search in MongoDB Atlas to retrieve relevant documents.\n",
    "- Combine the retrieved documents into a context for the LLM.\n",
    "- Construct an augmented prompt for an LLM.\n",
    "- Generate an answer using Azure OpenAI with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Complete Lab 1 and Lab 2 (MongoDB Atlas setup and data ingestion).\n",
    "- Ensure your MongoDB Atlas Vector Search index is built and ready.\n",
    "- Python environment set up with `pymongo`, `voyageai`, `langchain-openai`, and `python-dotenv` installed.\n",
    "- `.env` file containing the following variables:\n",
    "  - `MONGODB_URI`\n",
    "  - `VOYAGEAI_API_KEY`\n",
    "  - `AZURE_OPENAI_API_KEY`\n",
    "  - `AZURE_OPENAI_ENDPOINT`\n",
    "  - `AZURE_OPENAI_DEPLOYMENT_NAME`\n",
    "  - `AZURE_OPENAI_API_VERSION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai pymongo python-dotenv voyageai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Variables and Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Voyage-AI Client\n",
    "voyageai_api_key = os.environ.get(\"VOYAGEAI_API_KEY\")\n",
    "if not voyageai_api_key:\n",
    "    raise ValueError(\"VOYAGEAI_API_KEY not found in .env file or environment variables.\")\n",
    "vo = voyageai.Client(api_key=voyageai_api_key)\n",
    "\n",
    "# Initialize MongoDB Client\n",
    "mongo_uri = os.environ.get(\"MONGODB_URI\")\n",
    "if not mongo_uri:\n",
    "    raise ValueError(\"MONGODB_URI not found in .env file or environment variables.\")\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "# Select your database and collection\n",
    "db = client['rag_db']\n",
    "collection = db['documents']\n",
    "\n",
    "print(\"Clients initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define User Query and Embed It with Voyage-AI\n",
    "\n",
    "We'll take a sample user question and convert it into a vector embedding. Remember to specify `input_type=\"query\"` when embedding search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Tell me about the upcoming features of the software.\"\n",
    "print(f\"\\nUser Query: {user_query}\")\n",
    "\n",
    "print(\"Generating query embedding with Voyage-AI...\")\n",
    "try:\n",
    "    query_embedding_response = vo.embed(\n",
    "        texts=[user_query],\n",
    "        model=\"voyage-3-large\", # Use the same model as for indexing\n",
    "        input_type=\"query\" \n",
    "    )\n",
    "    query_embedding = query_embedding_response.embeddings[0]\n",
    "    print(f\"Query embedding generated. Dimension: {len(query_embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating query embedding: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform Vector Search in MongoDB Atlas\n",
    "\n",
    "Now we'll use the `$vectorSearch` aggregation stage to find documents in our MongoDB collection that are semantically similar to our `user_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "  {\n",
    "    '$vectorSearch': {\n",
    "      'queryVector': query_embedding,\n",
    "      'path': 'embedding',          # The field where your embeddings are stored\n",
    "      'numCandidates': 50,          # Number of nearest neighbors to search\n",
    "      'limit': 3,                   # Number of top results to return\n",
    "      'index': 'vector_index'            # Name of your Vector Search index (created in Lab 2)\n",
    "      # 'filter': { 'source': { '$eq': 'specific_document' } } # Optional: add filters\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    '$project': {\n",
    "      'text_chunk': 1,\n",
    "      'source': 1,\n",
    "      'score': { '$meta': 'vectorSearchScore' }, # Include similarity score\n",
    "      '_id': 0 # Exclude _id from results\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "print(\"Performing vector search in MongoDB Atlas...\")\n",
    "retrieved_documents = list(collection.aggregate(pipeline))\n",
    "\n",
    "if retrieved_documents:\n",
    "    print(f\"Retrieved {len(retrieved_documents)} relevant documents:\")\n",
    "    for doc in retrieved_documents:\n",
    "        print(f\"  - Score: {doc['score']:.4f}, Source: {doc['source']}, Text: {doc['text_chunk'][:70]}...\")\n",
    "else:\n",
    "    print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Context for the LLM\n",
    "\n",
    "We combine the `text_chunk` from the retrieved documents to form a single context string that will be passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([doc['text_chunk'] for doc in retrieved_documents])\n",
    "\n",
    "print(\"\\n--- Retrieved Context for LLM ---\")\n",
    "print(context)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "if not context:\n",
    "    print(\"Warning: No context was retrieved. The LLM might not be able to answer accurately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Construct Augmented Prompt for LLM\n",
    "\n",
    "We create the system instruction and user message that will be sent to Azure OpenAI. The system message tells the LLM how to behave, while the user message contains the retrieved context and the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"You are a helpful assistant. Answer the user's question based on the provided context only.\n",
    "If you cannot find the answer in the context, politely state that the information is not available.\"\"\"\n",
    "\n",
    "if context:\n",
    "    user_message = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "else:\n",
    "    user_message = f\"\"\"I couldn't find relevant information for the following question.\n",
    "Please state that the information is not available in the provided knowledge base.\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\n--- System Instruction ---\")\n",
    "print(system_instruction)\n",
    "print(\"\\n--- User Message ---\")\n",
    "print(user_message)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Answer with Azure OpenAI (LangChain)\n",
    "\n",
    "We use `AzureChatOpenAI` from the `langchain-openai` package to call your Azure OpenAI deployment. The model receives the system instruction and the augmented user message, then generates a grounded answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Read Azure OpenAI configuration from environment variables\n",
    "azure_openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "\n",
    "if not all([azure_openai_api_key, azure_openai_endpoint, azure_openai_deployment]):\n",
    "    raise ValueError(\n",
    "        \"Missing Azure OpenAI environment variables. \"\n",
    "        \"Please set AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, \"\n",
    "        \"and AZURE_OPENAI_DEPLOYMENT_NAME in your .env file.\"\n",
    "    )\n",
    "\n",
    "# Initialize Azure OpenAI LLM via LangChain\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    model=azure_openai_deployment,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Azure OpenAI LLM initialized (deployment: {azure_openai_deployment})\")\n",
    "\n",
    "# Build the messages list\n",
    "messages = [\n",
    "    SystemMessage(content=system_instruction),\n",
    "    HumanMessage(content=user_message),\n",
    "]\n",
    "\n",
    "# Invoke the LLM\n",
    "print(\"\\nCalling Azure OpenAI via LangChain...\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"\\n--- LLM Response ---\")\n",
    "print(response.content)\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully built a complete RAG pipeline! Here's what we accomplished:\n",
    "\n",
    "1. **Embedded** a user query using Voyage-AI.\n",
    "2. **Retrieved** relevant documents from MongoDB Atlas via Vector Search.\n",
    "3. **Constructed** an augmented prompt with retrieved context.\n",
    "4. **Generated** a grounded answer using Azure OpenAI through LangChain.\n",
    "\n",
    "In the next lab, we'll explore how to further improve retrieval quality using Voyage-AI's **reranking** capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to close the MongoDB client connection\n",
    "client.close()\n",
    "print(\"MongoDB client connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
