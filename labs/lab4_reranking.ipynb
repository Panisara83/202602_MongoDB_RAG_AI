{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Enhancing RAG with Voyage-AI Reranking\n",
    "\n",
    "This optional lab demonstrates how to integrate Voyage-AI's reranking capabilities into your RAG pipeline. Reranking helps to improve the precision of retrieved documents, ensuring that the most relevant information is passed to the LLM, leading to higher quality responses.\n",
    "\n",
    "## Objectives\n",
    "- Understand why reranking is beneficial in RAG.\n",
    "- Implement Voyage-AI's reranking API to re-order search results.\n",
    "- Observe the impact of reranking on the context provided to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Complete Lab 1, Lab 2, and Lab 3.\n",
    "- Python environment set up with `pymongo`, `voyageai`, and `python-dotenv` installed.\n",
    "- `.env` file containing `MONGODB_URI` and `VOYAGEAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Variables and Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Voyage-AI Client\n",
    "voyageai_api_key = os.environ.get(\"VOYAGEAI_API_KEY\")\n",
    "if not voyageai_api_key:\n",
    "    raise ValueError(\"VOYAGEAI_API_KEY not found in .env file or environment variables.\")\n",
    "vo = voyageai.Client(api_key=voyageai_api_key)\n",
    "\n",
    "# Initialize MongoDB Client\n",
    "mongodb_uri = os.environ.get(\"MONGODB_URI\")\n",
    "if not mongodb_uri:\n",
    "    raise ValueError(\"MONGODB_URI not found in .env file or environment variables.\")\n",
    "client = MongoClient(mongodb_uri)\n",
    "\n",
    "azure_openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "if not azure_openai_api_key:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY not found in .env file or environment variables.\")\n",
    "\n",
    "azure_openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "if not azure_openai_endpoint:\n",
    "    raise ValueError(\"AZURE_OPENAI_ENDPOINT not found in .env file or environment variables.\")\n",
    "\n",
    "azure_openai_api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-05-15\") # Default to a common version\n",
    "azure_openai_deployment_name = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "if not azure_openai_deployment_name:\n",
    "    raise ValueError(\"AZURE_OPENAI_DEPLOYMENT_NAME not found in .env file or environment variables.\")\n",
    "\n",
    "# Initialize Azure OpenAI LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_deployment=azure_openai_deployment_name,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Select your database and collection\n",
    "db = client['rag_db']\n",
    "collection = db['documents']\n",
    "\n",
    "print(\"Clients initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define User Query and Perform Initial Vector Search\n",
    "\n",
    "We start with a user query and perform an initial vector search, potentially retrieving more documents than strictly needed, as reranking will help us select the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What are the latest security features?\"\n",
    "print(f\"\\nUser Query: {user_query}\")\n",
    "\n",
    "print(\"Generating query embedding with Voyage-AI...\")\n",
    "try:\n",
    "    query_embedding_response = vo.embed(\n",
    "        texts=[user_query],\n",
    "        model=\"voyage-3-large\", \n",
    "        input_type=\"query\" \n",
    "    )\n",
    "    query_embedding = query_embedding_response.embeddings[0]\n",
    "    print(\"Query embedding generated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating query embedding: {e}\")\n",
    "    exit()\n",
    "\n",
    "pipeline = [\n",
    "  {\n",
    "    '$vectorSearch': {\n",
    "      'queryVector': query_embedding,\n",
    "      'path': 'embedding',          \n",
    "      'numCandidates': 30,         # Search more candidates for reranking\n",
    "      'limit': 10,                  # Retrieve top 10 for reranking\n",
    "      'index': 'vector_index'            \n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    '$project': {\n",
    "      'text_chunk': 1,\n",
    "      'source': 1,\n",
    "      'score': { '$meta': 'vectorSearchScore' },\n",
    "      '_id': 0 \n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "print(\"Performing initial vector search in MongoDB Atlas...\")\n",
    "initial_retrieved_documents = list(collection.aggregate(pipeline))\n",
    "\n",
    "if initial_retrieved_documents:\n",
    "    print(f\"Retrieved {len(initial_retrieved_documents)} initial documents for reranking.\")\n",
    "    for i, doc in enumerate(initial_retrieved_documents):\n",
    "        print(f\"  {i+1}. Score: {doc['score']:.4f}, Source: {doc['source']}, Text: {doc['text_chunk'][:50]}...\")\n",
    "else:\n",
    "    print(\"No initial documents found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Rerank Documents with Voyage-AI\n",
    "\n",
    "We pass the original `user_query` and the `text_chunk`s from our initial retrieval to Voyage-AI's reranker. It will return a new set of scores indicating how relevant each document is to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_rerank = [doc['text_chunk'] for doc in initial_retrieved_documents]\n",
    "\n",
    "print(\"\\nReranking documents with Voyage-AI...\")\n",
    "try:\n",
    "    rerank_result = vo.rerank(\n",
    "        query=user_query,\n",
    "        documents=documents_to_rerank,\n",
    "        model=\"rerank-2.5-lite\" # Or another reranking model\n",
    "    )\n",
    "\n",
    "    # Sort the original documents based on the new relevance scores\n",
    "    reranked_documents_with_scores = sorted(\n",
    "        zip(initial_retrieved_documents, rerank_result.results),\n",
    "        key=lambda x: x[1].relevance_score, \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"Reranked {len(reranked_documents_with_scores)} documents.\")\n",
    "    print(\"Top reranked documents:\")\n",
    "    for i, (original_doc, reranked_item) in enumerate(reranked_documents_with_scores[:5]): # Show top 5\n",
    "        print(f\"  {i+1}. Rerank Score: {reranked_item.relevance_score:.4f}, Original Score: {original_doc['score']:.4f}, Source: {original_doc['source']}, Text: {original_doc['text_chunk'][:50]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reranking documents: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Augmented Prompt with Reranked Context\n",
    "\n",
    "Now we take the top documents after reranking and use them to build the context for our LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_after_rerank = 3 # Choose how many top reranked documents to send to the LLM\n",
    "final_context_chunks = [doc[0]['text_chunk'] for doc in reranked_documents_with_scores[:top_n_after_rerank]]\n",
    "context_reranked = \"\\n\".join(final_context_chunks)\n",
    "\n",
    "print(\"\\n--- Reranked Context for LLM ---\")\n",
    "print(context_reranked)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "if not context_reranked:\n",
    "    print(\"Warning: No reranked context was available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if context_reranked:\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant. Answer the user's question based on the provided context only. If you cannot find the answer in the context, politely state that the information is not available.\"),\n",
    "            (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"context\": context_reranked, \"question\": user_query})\n",
    "    print(\"\\n--- LLM Augmented Prompt (with Reranking) ---\")\n",
    "    print(response.content)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "else:\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant. I couldn't find relevant information for the following question. Please state that the information is not available in the provided knowledge base.\"),\n",
    "            (\"human\", \"Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"question\": user_query})\n",
    "    print(\"\\n--- LLM Augmented Prompt (with Reranking) ---\")\n",
    "    print(response.content)\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Reranking with Voyage-AI provides an effective way to refine the search results before feeding them to an LLM, potentially leading to more accurate and relevant responses in your RAG application. This is a crucial step for optimizing performance in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run and Test (Azure OpenAI and Langchain Integration)\n",
    "\n",
    "To run this notebook and test the Azure OpenAI and Langchain integration, ensure you have the following environment variables set in your `.env` file or environment:\n",
    "\n",
    "- `MONGODB_URI`: Your MongoDB Atlas connection URI.\n",
    "- `VOYAGEAI_API_KEY`: Your Voyage-AI API key.\n",
    "- `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key.\n",
    "- `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint (e.g., `https://YOUR_RESOURCE_NAME.openai.azure.com/`).\n",
    "- `AZURE_OPENAI_DEPLOYMENT_NAME`: The name of your Azure OpenAI model deployment (e.g., `gpt-4`, `gpt-35-turbo`).\n",
    "- `AZURE_OPENAI_API_VERSION`: The API version for Azure OpenAI (e.g., `2023-05-15`). Defaulted to `2023-05-15` if not provided.\n",
    "\n",
    "**Steps to run:**\n",
    "1.  Ensure all prerequisites (including `langchain-openai`) are installed (`pip install -r requirements.txt` if you have one, or `!pip install langchain-openai --quiet` in a notebook cell).\n",
    "2.  Open the notebook in a Jupyter environment.\n",
    "3.  Run all cells sequentially. Observe the output from the Langchain-powered Azure OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to close the MongoDB client connection\n",
    "client.close()\n",
    "print(\"MongoDB client connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
